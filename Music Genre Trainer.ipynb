{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15045a63-10b5-48cd-8d3f-1f8964506c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Flatten,Dense,Dropout\n",
    "from tensorflow.keras.optimizers.legacy import Adam #for MAC configuration for windows use directly from optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df607933-9310-4944-b163-0f9ef20782cb",
   "metadata": {},
   "source": [
    "## Visualize single audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96715e-b2e8-472a-844c-2505d425277a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file = \"rock.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca4468c-fb12-4a1a-81b7-153c7a52ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(random_file, sr=44100)\n",
    "plt.figure(figsize=(14,5))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "# print(30*44100)\n",
    "# print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4da02f-7c25-4780-a5c8-32b330db77b8",
   "metadata": {},
   "source": [
    "## Play audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8696d81-5e1b-4db5-bea0-ce9c10482957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(data=y,rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe56d27-ced1-45c2-8e0c-ed3ebe997419",
   "metadata": {},
   "source": [
    "## Visualizing chunks of audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0dce20-44d2-485d-9bc4-028df1eb61f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"rock.wav\"\n",
    "y, sr = librosa.load(audio_path, sr=None)  # sr=None to keep the original sampling rate\n",
    "\n",
    "# Define the duration of each chunk and overlap\n",
    "chunk_duration = 4  # seconds\n",
    "overlap_duration = 2  # seconds To maintain the previous information of the audio sample\n",
    "\n",
    "# Convert durations to samples\n",
    "chunk_samples = chunk_duration * sr\n",
    "overlap_samples = overlap_duration * sr\n",
    "print(len(y), chunk_samples, sr)\n",
    "\n",
    "# Calculate the number of chunks\n",
    "#window size is of 2\n",
    "#First sample 0 to 4 then second sample is 2 to 6\n",
    "num_chunks = int(np.ceil((len(y) - chunk_samples) / (chunk_samples - overlap_samples))) + 1 \n",
    "print(num_chunks) #30 sec ko 15 huna parcha\n",
    "\n",
    "# Iterate over each chunk\n",
    "for i in range(num_chunks):\n",
    "    # Calculate start and end indices of the chunk\n",
    "    start = i * (chunk_samples - overlap_samples)\n",
    "    end = start + chunk_samples\n",
    "    \n",
    "    # Extract the chunk of audio\n",
    "    chunk = y[start:end]\n",
    "    plt.figure(figsize=(4, 2))\n",
    "    librosa.display.waveshow(chunk, sr=sr)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb473493-0ca3-403c-b7bc-01e542e5b730",
   "metadata": {},
   "source": [
    "## Melspectrogram Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37453860-8e18-4bf4-aa39-6b91792806d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_file_name = \"rock.wav\"\n",
    "y,sr = librosa.load(random_file_name,sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5e267f-3814-42e0-991c-d75ee464e117",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting Melspectrogram of Entire Audio\n",
    "def plot_melspectrogram(y,sr):\n",
    "    #Compute spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y,sr=sr)\n",
    "    #Convert to decibels (log scale)\n",
    "    spectrogram_db = librosa.power_to_db(spectrogram,ref=np.max)\n",
    "    #Visualize the spectrogram\n",
    "    plt.figure(figsize=(10,4))\n",
    "    librosa.display.specshow(spectrogram_db,sr=sr,x_axis='time',y_axis='mel')\n",
    "    plt.colorbar(format='%2.0f dB')\n",
    "    plt.title(\"Spectrogram\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11450a98-dc3a-413a-844c-dc67fd806585",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_melspectrogram(y, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876e7a59-6798-433b-b124-25dbee8fb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melspectrogram for audio chunks\n",
    "def plot_melspectrogram_chunks(y,sr):\n",
    "    #define the duration of each chunk and overlap\n",
    "    chunk_duration = 4\n",
    "    overlap_duration = 2\n",
    "    \n",
    "    #Convert duration to sample\n",
    "    chunk_samples = chunk_duration * sr\n",
    "    overlap_samples = overlap_duration * sr\n",
    "    \n",
    "    #Calculate the number of chunks\n",
    "    num_chunks = int(np.ceil((len(y)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "    \n",
    "    #iterate over each chunks\n",
    "    for i in range(num_chunks):\n",
    "        #Calculate start and end indices of the chunk\n",
    "        start = i*(chunk_samples-overlap_samples)\n",
    "        end = start+chunk_samples\n",
    "        #Extract the chunk audio\n",
    "        chunk = y[start:end]\n",
    "        #Melspectrogram part\n",
    "        spectrogram = librosa.feature.melspectrogram(y=chunk,sr=sr)\n",
    "        print(spectrogram.shape)\n",
    "        spectrogram_db = librosa.power_to_db(spectrogram,ref=np.max)\n",
    "        #Visualize the spectrogram\n",
    "        plt.figure(figsize=(10,4))\n",
    "        librosa.display.specshow(spectrogram_db,sr=sr,x_axis='time',y_axis='mel')\n",
    "        plt.colorbar(format='%2.0f dB')\n",
    "        plt.title(\"Spectrogram\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202289d-4fcf-486f-8ca3-ed79759ae8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_melspectrogram_chunks(y,sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cc4cb-4be9-4ac6-a771-28657a980010",
   "metadata": {},
   "outputs": [],
   "source": [
    "128*345 #take close enough dimension to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e906658e-1866-433a-a3c6-dd06860da61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "210*210"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7be71-73c9-4f84-9710-047f25aa9882",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4094dbd-94c3-44af-8c77-cab8bb038a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"genres_original\"\n",
    "# classes = ['blues', 'classical','country','disco','hiphop','metal','pop','reggae','rock']\n",
    "classes = ['hiphop','metal','rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e03d5-8019-4df9-b334-7efe4a195e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.image import resize #resize for the above output dimension\n",
    "#Load and preprocess audio data\n",
    "def load_and_preprocess_data(data_dir,classes,target_shape=(150,150)):\n",
    "    data=[]\n",
    "    labels=[]\n",
    "\n",
    "    for i_class,class_name in enumerate(classes):\n",
    "        class_dir = os.path.join(data_dir,class_name)\n",
    "        print(\"Processing--\",class_name)\n",
    "        for filename in os.listdir(class_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                file_path = os.path.join(class_dir,filename)\n",
    "                audio_data,sample_rate = librosa.load(file_path,sr=None)\n",
    "                #Performing Preprocessing\n",
    "                #define the duration of each chunk and overlap\n",
    "                chunk_duration = 4\n",
    "                overlap_duration = 2\n",
    "                \n",
    "                #Convert duration to sample\n",
    "                chunk_samples = chunk_duration * sample_rate\n",
    "                overlap_samples = overlap_duration * sample_rate\n",
    "                \n",
    "                #Calculate the number of chunks\n",
    "                num_chunks = int(np.ceil((len(audio_data)-chunk_samples)/(chunk_samples-overlap_samples)))+1\n",
    "                \n",
    "                #iterate over each chunks\n",
    "                for i in range(num_chunks):\n",
    "                    #Calculate start and end indices of the chunk\n",
    "                    start = i*(chunk_samples-overlap_samples)\n",
    "                    end = start+chunk_samples\n",
    "                    #Extract the chunk audio\n",
    "                    chunk = audio_data[start:end]\n",
    "                    #Melspectrogram part\n",
    "                    mel_spectrogram = librosa.feature.melspectrogram(y=chunk,sr=sample_rate)\n",
    "                    #Resize matrix based on provided target shape 150 x 150\n",
    "                    mel_spectrogram = resize(np.expand_dims(mel_spectrogram,axis=-1),target_shape)\n",
    "                    #Append data to list\n",
    "                    data.append(mel_spectrogram)\n",
    "                    labels.append(i_class)\n",
    "    #Return\n",
    "    return np.array(data),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f2734b-db6a-4218-8a08-9532f82a0b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels = load_and_preprocess_data(data_dir,classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c217bb93-7c43-41a2-8275-89b2581b3cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e369e-ca27-4f84-b2af-eed356c069b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029ae32-0f1e-4f91-8172-4ce1b663b8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "#one hot encoding\n",
    "labels = to_categorical(labels,num_classes = len(classes)) # Converting labels to one-hot encoding\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60421aaa-4963-40e8-8062-a4a9327a2e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b606c0ea-090b-4437-8dde-4a841ef002a5",
   "metadata": {},
   "source": [
    "## Splitting dataset into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d1c51c-a464-48ae-b5fb-fc66edc285e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#80% training, 20% test set\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(data,labels,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3511f793-c25c-4188-a2ec-6473c912c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aea198-63f7-44dd-b9a1-9e0ffda9903f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eef1b04-bc92-4777-8e5a-b0bccc4d889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155e785-aad8-47c4-8e3d-62b1c4421e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1c3be5-4546-436c-982b-f0cd86f4f587",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93252780-c2b8-464a-9a2d-31a120c58486",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c64dc5-1bad-4a6b-a19c-67d2ae57ea6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d250a89-43fa-4f28-bb13-5cf7d5e80f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding to preserve data after using filter\n",
    "model.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=X_train[0].shape))\n",
    "#second layer to remove unwanted info\n",
    "model.add(Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "# max pooling use same as second layer but is more specific\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37d826-c337-4c65-93ce-c19cf214e794",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=64,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e46743a-ff02-45a2-a733-4655c8b236df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=128,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb463d6c-7db6-42ad-b91c-d65da8e83eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfa661-57d3-45ac-b6bc-15d62a49fbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=256,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e312934c-bf96-4c79-ad8c-35f3c519baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=512,kernel_size=3,padding='same',activation='relu'))\n",
    "model.add(Conv2D(filters=512,kernel_size=3,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f652d08-f162-4976-8e14-971d323bca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.3)) #drop 30% of neurons from that to deal with overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba60413a-117b-4351-9f74-64f6829d22c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9d35a5-a866-427c-9cb8-a67b60e778ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(units=1200,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b8b66d-f3ef-4278-80d5-f671526d0e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29e5058-7244-43ed-8402-f4d8bef8c9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output layer\n",
    "model.add(Dense(units=len(classes),activation='softmax')) #multiclass = softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c7cd84-9a38-411d-b3af-8e2dae0a61e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8373241-85ff-4600-9437-1a49d8fbbadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile the model\n",
    "#multi class = categorical_crossentropy\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723be2b4-8cfc-49ef-9195-756ea23c65f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Model\n",
    "training_history = model.fit(X_train,Y_train,epochs=30,batch_size=32,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2fb2a-8d37-4055-a7e6-57aeda76bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Trained_model.keras\") #Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1348b-dc5d-49e2-8fdd-3ca680fcda29",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3404ec80-621e-4a2b-9933-01dd0c3b943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recording History in json\n",
    "import json\n",
    "with open('training_hist.json','w') as f:\n",
    "  json.dump(training_history.history,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e57778-04bd-43e5-b297-cad871a5fe9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ccce2e-6e6f-4896-92da-053e19f2b556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 32)      320       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 148, 148, 32)      9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 74, 74, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 74, 74, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 72, 72, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 36, 36, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 36, 36, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 34, 34, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 17, 17, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 17, 17, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 17, 17, 256)       295168    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 15, 15, 256)       590080    \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 7, 7, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 7, 7, 512)         1180160   \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 5, 5, 512)         2359808   \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 2, 2, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1200)              2458800   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1200)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 3603      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7174051 (27.37 MB)\n",
      "Trainable params: 7174051 (27.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Reloading model variable\n",
    "model = tf.keras.models.load_model(\"Trained_model.keras\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35efd820-04b1-49d6-aea4-b5ad20ede2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reloading Training history\n",
    "import json\n",
    "with open(\"training_hist.json\",'r') as json_file:\n",
    "    training_history_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abdc7e6-ed2a-46c6-a546-b709ddb8d0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269259a9-db6d-4c26-95df-73b0d6a3fb97",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cfe477-1d72-4ef2-84f4-bb026d13aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation on Training set\n",
    "train_loss,train_accuracy = model.evaluate(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b21e33-b7f9-442e-9be0-d81f7e166dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss,train_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663eff0-f891-425c-8181-477b9b8198c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation on Validation set\n",
    "val_loss,val_accuracy = model.evaluate(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acc6a3-4ef9-4a7a-88ba-17dc57a7d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss,val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a400bec-54f1-4553-8864-6b3b5f8927ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history_data['val_loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413d401-03b6-4081-9cfc-fb823d2109ff",
   "metadata": {},
   "source": [
    "## Accuracy and Loss Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aad268-eee3-419b-a149-64b37c76c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Loss\n",
    "epochs = [i for i in range(1,31)]\n",
    "plt.plot(epochs,training_history_data['loss'],label=\"Training Loss\",color='red')\n",
    "plt.plot(epochs,training_history_data['val_loss'],label=\"Validation Loss\",color='blue')\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Visualization of Loss Result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8089529-093d-493e-8e3a-3a32facad18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of Accuracy\n",
    "epochs = [i for i in range(1,31)]\n",
    "plt.plot(epochs,training_history_data['accuracy'],label=\"Training Accuracy\",color='red')\n",
    "plt.plot(epochs,training_history_data['val_accuracy'],label=\"Validation Accuracy\",color='blue')\n",
    "plt.xlabel(\"No. of Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Visualization of Accuracy Result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5472940f-a080-418c-b688-a2a8747007b1",
   "metadata": {},
   "source": [
    "## Precision, Recall, Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b208a4b-3f32-41cd-a192-bbe466900e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e14d67-eb87-484c-afe2-9676774a264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad95ee5-b47e-4c73-a343-f04f080e2d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3293387f-7dde-4846-96e3-27345683edd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categories = np.argmax(y_pred,axis=1)\n",
    "predicted_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0845fb7-8e3a-4cc2-a20b-7fd445731bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec8d4a-1cb6-4e65-8a01-87c8996a6a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061fba5-0a1e-4f2a-87df-9717816ffa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_categories = np.argmax(Y_test,axis=1)\n",
    "true_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81feffd0-4571-49b4-9f20-78c36cb51c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b081dc-2d2b-4350-8ea0-9501abc1e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm = confusion_matrix(true_categories,predicted_categories)\n",
    "# Precision Recall F1score\n",
    "print(classification_report(true_categories,predicted_categories,target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cbb850-e3c4-485e-88f3-a0d8a24af51d",
   "metadata": {},
   "source": [
    "## Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913a55d0-eb4d-4dff-b0f6-6e35574c2871",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc45e0-2faf-4814-8fe1-a1082c932ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(15,15))\n",
    "sns.heatmap(cm,annot=True,annot_kws={\"size\":10})\n",
    "plt.xlabel(\"Predicted Class\",fontsize=10)\n",
    "plt.ylabel(\"Actual Class\",fontsize=10)\n",
    "plt.title(\"Music Genre Classification Confusion Matrix\",fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadf6e1c-99d3-4434-a629-26ff8fe56021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
